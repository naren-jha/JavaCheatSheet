Dinic's algorithm is one of those
extremely fast and revolutionary
algorithms which really pushed the field
of network flow forwards.
it was one if
not the first algorithm to introduce a
bunch of new concepts like building a
level graph, combining multiple graph
traversal techniques together, and the
concept of a blocking flow.


Dinic's algorithm is a fast a strongly
polynomial maximum flow algorithm. 
the
fact that it's strongly polynomial is
important, because
it means that the runtime of the algorithm
doesn't depend on the capacity values of
the flow graph. 
time complexity of Dinic's algorithm is O(EV2).


what's remarkable
about Dinic's is that not only is it fast
in practice for general graphs, but it
boosts performance on bipartite graphs - 
running in a time complexity of O(âˆšVE)
the importance of
this cannot be overstated. 
it makes it
possible to handle bipartite graphs of a
ridiculous size. 
if you're doing
competitive programming, Dinic's is the 
de facto standard algorithm to solve
maximum flow problems. 


the algorithm
was conceived in 1969 by Yefim Dinitz
and published in 1970. 
it was
later modified slightly and popularized
by Shimon Evan, mispronouncing Dinitz
as Dinic's. 


To understand the intuition behind the algorithm,
let's begin by making an analogy.
suppose you and your friend are planning to
meet up at a coffee shop - a few streets
east of where you are. 


you've never been
to this coffee shop and you don't
exactly know where it is.
but you know
it's somewhere East. 


so how would you get
there? 
with the information you have,
would it make sense to
South? what about North-West? 
actually the only
sensible directions are East, North-East
and South-East. 


this is because you know
that those directions guarantee that you
make a positive progress towards the
coffee shop. 
this form of heuristic
ensures that we continuously make
progress towards whatever place of
interest we desire to go. 


so how can we
apply this concept while solving the
maximum flow problem? 
in this analogy YOU are the
source node and the COFFEE SHOP is the
sink. 
the main idea behind Dinic's
algorithm is to guide augmenting paths
from the source to the sink, using a
level graph, and in doing so greatly
reducing the runtime. 


the way Dinic's
determines what edges make progress
towards the sink t, and which do not, is
by building what's called a level graph.
the levels of a graph are those obtained
by doing a Breadth-First Search from the
source. 
furthermore, an edge is only part
of the level graph if it makes progress
towards the sink. 
that means an edge must go
from a node at level L to another node
at level L + 1. 


the requirement that
edges must go from L to L + 1 prunes
backwards or what we can call sideways edges.
those are all the gray edges in the
figure below. 


as an example, if we're trying
to get from s to t as quickly as
possible, does it make sense to take the
red edge going in the backwards
direction (see figure below)? No! it does not. 
taking the red
edge doesn't bring us any closer to the
sink, so it should only be taken if a
detour is required. 
this is why backward
edges are omitted from the level graph.


the same thing can be said about edges
which cut across sideways, across the
same level, since no progress is made.


it's also worth mentioning that residual
edges can be made part of the level
graph but they must have a remaining
capacity greater than zero. 


so that was the
level graph. 
the actual steps to
executing Dinic's
are as follows - 


1. first construct a level
graph by doing a Breadth-First Search
from the source to label all the levels
of the current flow graph. 


2. if the
sink was never reached while building
the level graph, then we can stop
and return the value of the maximum flow.


3. then using only valid edges in the level
graph, do multiple Depth-First Searches
from the source to the sink until a
blocking flow is reached. 
and sum over
the bottleneck values of all augmenting
paths to calculate the maximum flow 


then repeat steps 1 to 3.


a blocking
flow is when we cannot find any more
paths from the source to the sink
because too many edges in the level
graph have been saturated.


Now that we understand the algorithm on high level,
let's clear things out with an example.
Let's find maximum flow for the following flow graph using 
Dinic's algorithm.
if this was a
bipartite graph we would also be able to
get a maximum matching as a result. 


step one is to figure out which
edges are part of the current level
graph. 
we don't need to think of the
level graph as a totally separate graph.
we can think of it as a subset
of the edges. 


so we start at the source
and do a breadth-first search outwards.
the first layer includes all the red
nodes, second layer the green nodes and
the third layer the purple nodes as shown in figure below.


now if we
focus on the edges which form the level
graph, we can see that they are all the edges
which go from L to L + 1 in level
and have a remaining capacity > 0 


step two of the algorithm is
to find paths from s to t
until a blocking flow is reached
(i.e., when we cannot find any more paths through
the level graph). 


so we start at the
source and do a Depth-First Search on
the edges of level graph until the sink
is reached. 


below figure shows our first
augmenting path in the level graph.
the bottleneck value
along this path is 5, since 5 is the
smallest remaining capacity. 


so we update
the flow values along the path by 5. 


if we inspect the graph the blocking flow
has not yet been reached, since there
still exists paths from s to t. 


so we start once
again at the source and do a Depth-First
Search forwards. 
Below figure shows another augmenting
path from s to t. 
this one has a bottleneck value of 15. 


so augment the flow along the path by
15 units. 


now let's try to find another
path from s to t. 
what happens now is
that we get stuck performing the
Depth-First Search. 
there are no edges in
the level graph with a remaining
capacity > 0, which can
lead us to the sink. 
so the blocking flow
has been reached. 


we just finished the
first blocking flow iteration. 
now we
reset and rebuild the level graph. 
this
time it should look different because
the remaining capacities of multiple
edges has changed. 


we start at the source again,
expand outwards taking all edges with a
remaining capacity > 0,
which in this case is only the middle
edge leading us to the red node. 
the top
edge going outwards from the source is
saturated and so is the one going
downwards.

 
we keep doing this and
building the level graph layer by layer.


so this is our new level graph.
we can see that this time we have one
extra layer. let's try and
find a path from s to t. 


once again, we
start at the source and probe forwards
using only the edges part of the level graph.


we have reached a dead end in
our Depth-First Search.
so what we need to do is
backtrack and keep going until we reach
the sink.


we made it to the sink. 
the
current path has a bottleneck value of
10. 


so we augment the flow by 10 units 


now if we inspect the flow graph we
notice that the blocking flow has
once again been reached. 


now no more flow
can be pushed through the network when
we build a level graph.
which means the
algorithm terminates. 


the maximum flow is
the sum of all the bottleneck values.
which in this case is = 5 + 15 + 10 = 30


We can also compute the maximum flow by summing the flow values going into the sink.


one of the pitfalls of the
current implementation of Dinic's
algorithm is that it may
encounter multiple dead ends during a
Depth-First Search phase. 
this is
especially bad if the same dead end is
taken multiple times during a blocking
flow iteration. 


to resolve this issue, in
his original paper, Dinitz suggested
cleaning the level graph and getting rid
of all the dead ends, before beginning with each
blocking flow phase. 


then later in 1975
Shimon Evan suggested pruning dead ends
when backtracking during the Depth-First
Search phase, effectively getting rid of
dead ends on the fly as the algorithm
executes. 
this trick greatly speeds up
and simplifies the algorithm because
dead ends are only ever encountered
once. 


